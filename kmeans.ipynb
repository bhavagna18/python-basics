{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9oRYR3YCKN9XT4uIWcUVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavagna18/python-basics/blob/main/kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wLEAtUV2NV96"
      },
      "outputs": [],
      "source": [
        "#Import the libraries\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "xCV0sg_PNhcS",
        "outputId": "f0aca010-8c95-4ad5-9288-bdc418a3d1a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = ('/content/drive')\n",
        "df = pd.read_csv"
      ],
      "metadata": {
        "id": "i4YMxC9GOT2W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "gTgfu4okOwx0",
        "outputId": "e5d46bab-917d-48fd-d383-d189ddbd7596"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function pandas.io.parsers.readers.read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.io.parsers.readers.read_csv</b><br/>def read_csv(filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], *, sep: str | None | lib.NoDefault=lib.no_default, delimiter: str | None | lib.NoDefault=None, header: int | Sequence[int] | None | Literal[&#x27;infer&#x27;]=&#x27;infer&#x27;, names: Sequence[Hashable] | None | lib.NoDefault=lib.no_default, index_col: IndexLabel | Literal[False] | None=None, usecols: UsecolsArgType=None, dtype: DtypeArg | None=None, engine: CSVEngine | None=None, converters: Mapping[Hashable, Callable] | None=None, true_values: list | None=None, false_values: list | None=None, skipinitialspace: bool=False, skiprows: list[int] | int | Callable[[Hashable], bool] | None=None, skipfooter: int=0, nrows: int | None=None, na_values: Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None=None, keep_default_na: bool=True, na_filter: bool=True, verbose: bool | lib.NoDefault=lib.no_default, skip_blank_lines: bool=True, parse_dates: bool | Sequence[Hashable] | None=None, infer_datetime_format: bool | lib.NoDefault=lib.no_default, keep_date_col: bool | lib.NoDefault=lib.no_default, date_parser: Callable | lib.NoDefault=lib.no_default, date_format: str | dict[Hashable, str] | None=None, dayfirst: bool=False, cache_dates: bool=True, iterator: bool=False, chunksize: int | None=None, compression: CompressionOptions=&#x27;infer&#x27;, thousands: str | None=None, decimal: str=&#x27;.&#x27;, lineterminator: str | None=None, quotechar: str=&#x27;&quot;&#x27;, quoting: int=csv.QUOTE_MINIMAL, doublequote: bool=True, escapechar: str | None=None, comment: str | None=None, encoding: str | None=None, encoding_errors: str | None=&#x27;strict&#x27;, dialect: str | csv.Dialect | None=None, on_bad_lines: str=&#x27;error&#x27;, delim_whitespace: bool | lib.NoDefault=lib.no_default, low_memory: bool=_c_parser_defaults[&#x27;low_memory&#x27;], memory_map: bool=False, float_precision: Literal[&#x27;high&#x27;, &#x27;legacy&#x27;] | None=None, storage_options: StorageOptions | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default) -&gt; DataFrame | TextFileReader</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py</a>Read a comma-separated values (csv) file into DataFrame.\n",
              "\n",
              "Also supports optionally iterating or breaking of the file\n",
              "into chunks.\n",
              "\n",
              "Additional help can be found in the online docs for\n",
              "`IO Tools &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html&gt;`_.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "filepath_or_buffer : str, path object or file-like object\n",
              "    Any valid string path is acceptable. The string could be a URL. Valid\n",
              "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
              "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
              "\n",
              "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
              "\n",
              "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
              "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
              "sep : str, default &#x27;,&#x27;\n",
              "    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
              "    C engine cannot automatically detect\n",
              "    the separator, but the Python parsing engine can, meaning the latter will\n",
              "    be used and automatically detect the separator from only the first valid\n",
              "    row of the file by Python&#x27;s builtin sniffer tool, ``csv.Sniffer``.\n",
              "    In addition, separators longer than 1 character and different from\n",
              "    ``&#x27;\\s+&#x27;`` will be interpreted as regular expressions and will also force\n",
              "    the use of the Python parsing engine. Note that regex delimiters are prone\n",
              "    to ignoring quoted data. Regex example: ``&#x27;\\r\\t&#x27;``.\n",
              "delimiter : str, optional\n",
              "    Alias for ``sep``.\n",
              "header : int, Sequence of int, &#x27;infer&#x27; or None, default &#x27;infer&#x27;\n",
              "    Row number(s) containing column labels and marking the start of the\n",
              "    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
              "    are passed the behavior is identical to ``header=0`` and column\n",
              "    names are inferred from the first line of the file, if column\n",
              "    names are passed explicitly to ``names`` then the behavior is identical to\n",
              "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
              "    replace existing names. The header can be a list of integers that\n",
              "    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
              "    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
              "    skipped (e.g. 2 in this example is skipped). Note that this\n",
              "    parameter ignores commented lines and empty lines if\n",
              "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
              "    data rather than the first line of the file.\n",
              "names : Sequence of Hashable, optional\n",
              "    Sequence of column labels to apply. If the file contains a header row,\n",
              "    then you should explicitly pass ``header=0`` to override the column names.\n",
              "    Duplicates in this list are not allowed.\n",
              "index_col : Hashable, Sequence of Hashable or False, optional\n",
              "  Column(s) to use as row label(s), denoted either by column labels or column\n",
              "  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
              "  will be formed for the row labels.\n",
              "\n",
              "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
              "  column as the index, e.g., when you have a malformed file with delimiters at\n",
              "  the end of each line.\n",
              "usecols : Sequence of Hashable or Callable, optional\n",
              "    Subset of columns to select, denoted either by column labels or column indices.\n",
              "    If list-like, all elements must either\n",
              "    be positional (i.e. integer indices into the document columns) or strings\n",
              "    that correspond to column names provided either by the user in ``names`` or\n",
              "    inferred from the document header row(s). If ``names`` are given, the document\n",
              "    header row(s) are not taken into account. For example, a valid list-like\n",
              "    ``usecols`` parameter would be ``[0, 1, 2]`` or ``[&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;]``.\n",
              "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
              "    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
              "    preserved use ``pd.read_csv(data, usecols=[&#x27;foo&#x27;, &#x27;bar&#x27;])[[&#x27;foo&#x27;, &#x27;bar&#x27;]]``\n",
              "    for columns in ``[&#x27;foo&#x27;, &#x27;bar&#x27;]`` order or\n",
              "    ``pd.read_csv(data, usecols=[&#x27;foo&#x27;, &#x27;bar&#x27;])[[&#x27;bar&#x27;, &#x27;foo&#x27;]]``\n",
              "    for ``[&#x27;bar&#x27;, &#x27;foo&#x27;]`` order.\n",
              "\n",
              "    If callable, the callable function will be evaluated against the column\n",
              "    names, returning names where the callable function evaluates to ``True``. An\n",
              "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
              "    [&#x27;AAA&#x27;, &#x27;BBB&#x27;, &#x27;DDD&#x27;]``. Using this parameter results in much faster\n",
              "    parsing time and lower memory usage.\n",
              "dtype : dtype or dict of {Hashable : dtype}, optional\n",
              "    Data type(s) to apply to either the whole dataset or individual columns.\n",
              "    E.g., ``{&#x27;a&#x27;: np.float64, &#x27;b&#x27;: np.int32, &#x27;c&#x27;: &#x27;Int64&#x27;}``\n",
              "    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
              "    to preserve and not interpret ``dtype``.\n",
              "    If ``converters`` are specified, they will be applied INSTEAD\n",
              "    of ``dtype`` conversion.\n",
              "\n",
              "    .. versionadded:: 1.5.0\n",
              "\n",
              "        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
              "        the default determines the ``dtype`` of the columns which are not explicitly\n",
              "        listed.\n",
              "engine : {&#x27;c&#x27;, &#x27;python&#x27;, &#x27;pyarrow&#x27;}, optional\n",
              "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
              "    is currently more feature-complete. Multithreading is currently only supported by\n",
              "    the pyarrow engine.\n",
              "\n",
              "    .. versionadded:: 1.4.0\n",
              "\n",
              "        The &#x27;pyarrow&#x27; engine was added as an *experimental* engine, and some features\n",
              "        are unsupported, or may not work correctly, with this engine.\n",
              "converters : dict of {Hashable : Callable}, optional\n",
              "    Functions for converting values in specified columns. Keys can either\n",
              "    be column labels or column indices.\n",
              "true_values : list, optional\n",
              "    Values to consider as ``True`` in addition to case-insensitive variants of &#x27;True&#x27;.\n",
              "false_values : list, optional\n",
              "    Values to consider as ``False`` in addition to case-insensitive variants of &#x27;False&#x27;.\n",
              "skipinitialspace : bool, default False\n",
              "    Skip spaces after delimiter.\n",
              "skiprows : int, list of int or Callable, optional\n",
              "    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
              "    at the start of the file.\n",
              "\n",
              "    If callable, the callable function will be evaluated against the row\n",
              "    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
              "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
              "skipfooter : int, default 0\n",
              "    Number of lines at bottom of file to skip (Unsupported with ``engine=&#x27;c&#x27;``).\n",
              "nrows : int, optional\n",
              "    Number of rows of file to read. Useful for reading pieces of large files.\n",
              "na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
              "    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
              "    per-column ``NA`` values.  By default the following values are interpreted as\n",
              "    ``NaN``: &quot; &quot;, &quot;#N/A&quot;, &quot;#N/A N/A&quot;, &quot;#NA&quot;, &quot;-1.#IND&quot;, &quot;-1.#QNAN&quot;, &quot;-NaN&quot;, &quot;-nan&quot;,\n",
              "    &quot;1.#IND&quot;, &quot;1.#QNAN&quot;, &quot;&lt;NA&gt;&quot;, &quot;N/A&quot;, &quot;NA&quot;, &quot;NULL&quot;, &quot;NaN&quot;, &quot;None&quot;,\n",
              "    &quot;n/a&quot;, &quot;nan&quot;, &quot;null &quot;.\n",
              "\n",
              "keep_default_na : bool, default True\n",
              "    Whether or not to include the default ``NaN`` values when parsing the data.\n",
              "    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
              "\n",
              "    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
              "      is appended to the default ``NaN`` values used for parsing.\n",
              "    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
              "      the default ``NaN`` values are used for parsing.\n",
              "    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
              "      the ``NaN`` values specified ``na_values`` are used for parsing.\n",
              "    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
              "      strings will be parsed as ``NaN``.\n",
              "\n",
              "    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
              "    ``na_values`` parameters will be ignored.\n",
              "na_filter : bool, default True\n",
              "    Detect missing value markers (empty strings and the value of ``na_values``). In\n",
              "    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
              "    performance of reading a large file.\n",
              "verbose : bool, default False\n",
              "    Indicate number of ``NA`` values placed in non-numeric columns.\n",
              "\n",
              "    .. deprecated:: 2.2.0\n",
              "skip_blank_lines : bool, default True\n",
              "    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
              "parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
              "    The behavior is as follows:\n",
              "\n",
              "    * ``bool``. If ``True`` -&gt; try parsing the index. Note: Automatically set to\n",
              "      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
              "    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -&gt; try parsing columns 1, 2, 3\n",
              "      each as a separate date column.\n",
              "    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -&gt; combine columns 1 and 3 and parse\n",
              "      as a single date column. Values are joined with a space before parsing.\n",
              "    * ``dict``, e.g. ``{&#x27;foo&#x27; : [1, 3]}`` -&gt; parse columns 1, 3 as date and call\n",
              "      result &#x27;foo&#x27;. Values are joined with a space before parsing.\n",
              "\n",
              "    If a column or index cannot be represented as an array of ``datetime``,\n",
              "    say because of an unparsable value or a mixture of timezones, the column\n",
              "    or index will be returned unaltered as an ``object`` data type. For\n",
              "    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
              "    :func:`~pandas.read_csv`.\n",
              "\n",
              "    Note: A fast-path exists for iso8601-formatted dates.\n",
              "infer_datetime_format : bool, default False\n",
              "    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
              "    format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
              "    switch to a faster method of parsing them. In some cases this can increase\n",
              "    the parsing speed by 5-10x.\n",
              "\n",
              "    .. deprecated:: 2.0.0\n",
              "        A strict version of this argument is now the default, passing it has no effect.\n",
              "\n",
              "keep_date_col : bool, default False\n",
              "    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
              "    keep the original columns.\n",
              "date_parser : Callable, optional\n",
              "    Function to use for converting a sequence of string columns to an array of\n",
              "    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
              "    conversion. pandas will try to call ``date_parser`` in three different ways,\n",
              "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
              "    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
              "    string values from the columns defined by ``parse_dates`` into a single array\n",
              "    and pass that; and 3) call ``date_parser`` once for each row using one or\n",
              "    more strings (corresponding to the columns defined by ``parse_dates``) as\n",
              "    arguments.\n",
              "\n",
              "    .. deprecated:: 2.0.0\n",
              "       Use ``date_format`` instead, or read in as ``object`` and then apply\n",
              "       :func:`~pandas.to_datetime` as-needed.\n",
              "date_format : str or dict of column -&gt; format, optional\n",
              "    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
              "    The strftime to parse time, e.g. :const:`&quot;%d/%m/%Y&quot;`. See\n",
              "    `strftime documentation\n",
              "    &lt;https://docs.python.org/3/library/datetime.html\n",
              "    #strftime-and-strptime-behavior&gt;`_ for more information on choices, though\n",
              "    note that :const:`&quot;%f&quot;` will parse all the way up to nanoseconds.\n",
              "    You can also pass:\n",
              "\n",
              "    - &quot;ISO8601&quot;, to parse any `ISO8601 &lt;https://en.wikipedia.org/wiki/ISO_8601&gt;`_\n",
              "        time string (not necessarily in exactly the same format);\n",
              "    - &quot;mixed&quot;, to infer the format for each element individually. This is risky,\n",
              "        and you should probably use it along with `dayfirst`.\n",
              "\n",
              "    .. versionadded:: 2.0.0\n",
              "dayfirst : bool, default False\n",
              "    DD/MM format dates, international and European format.\n",
              "cache_dates : bool, default True\n",
              "    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
              "    conversion. May produce significant speed-up when parsing duplicate\n",
              "    date strings, especially ones with timezone offsets.\n",
              "\n",
              "iterator : bool, default False\n",
              "    Return ``TextFileReader`` object for iteration or getting chunks with\n",
              "    ``get_chunk()``.\n",
              "chunksize : int, optional\n",
              "    Number of lines to read from the file per chunk. Passing a value will cause the\n",
              "    function to return a ``TextFileReader`` object for iteration.\n",
              "    See the `IO Tools docs\n",
              "    &lt;https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking&gt;`_\n",
              "    for more information on ``iterator`` and ``chunksize``.\n",
              "\n",
              "compression : str or dict, default &#x27;infer&#x27;\n",
              "    For on-the-fly decompression of on-disk data. If &#x27;infer&#x27; and &#x27;filepath_or_buffer&#x27; is\n",
              "    path-like, then detect compression from the following extensions: &#x27;.gz&#x27;,\n",
              "    &#x27;.bz2&#x27;, &#x27;.zip&#x27;, &#x27;.xz&#x27;, &#x27;.zst&#x27;, &#x27;.tar&#x27;, &#x27;.tar.gz&#x27;, &#x27;.tar.xz&#x27; or &#x27;.tar.bz2&#x27;\n",
              "    (otherwise no compression).\n",
              "    If using &#x27;zip&#x27; or &#x27;tar&#x27;, the ZIP file must contain only one data file to be read in.\n",
              "    Set to ``None`` for no decompression.\n",
              "    Can also be a dict with key ``&#x27;method&#x27;`` set\n",
              "    to one of {``&#x27;zip&#x27;``, ``&#x27;gzip&#x27;``, ``&#x27;bz2&#x27;``, ``&#x27;zstd&#x27;``, ``&#x27;xz&#x27;``, ``&#x27;tar&#x27;``} and\n",
              "    other key-value pairs are forwarded to\n",
              "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
              "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
              "    ``tarfile.TarFile``, respectively.\n",
              "    As an example, the following could be passed for Zstandard decompression using a\n",
              "    custom compression dictionary:\n",
              "    ``compression={&#x27;method&#x27;: &#x27;zstd&#x27;, &#x27;dict_data&#x27;: my_compression_dict}``.\n",
              "\n",
              "    .. versionadded:: 1.5.0\n",
              "        Added support for `.tar` files.\n",
              "\n",
              "    .. versionchanged:: 1.4.0 Zstandard support.\n",
              "\n",
              "thousands : str (length 1), optional\n",
              "    Character acting as the thousands separator in numerical values.\n",
              "decimal : str (length 1), default &#x27;.&#x27;\n",
              "    Character to recognize as decimal point (e.g., use &#x27;,&#x27; for European data).\n",
              "lineterminator : str (length 1), optional\n",
              "    Character used to denote a line break. Only valid with C parser.\n",
              "quotechar : str (length 1), optional\n",
              "    Character used to denote the start and end of a quoted item. Quoted\n",
              "    items can include the ``delimiter`` and it will be ignored.\n",
              "quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
              "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
              "    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
              "    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
              "    or ``lineterminator``.\n",
              "doublequote : bool, default True\n",
              "   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
              "   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
              "   field as a single ``quotechar`` element.\n",
              "escapechar : str (length 1), optional\n",
              "    Character used to escape other characters.\n",
              "comment : str (length 1), optional\n",
              "    Character indicating that the remainder of line should not be parsed.\n",
              "    If found at the beginning\n",
              "    of a line, the line will be ignored altogether. This parameter must be a\n",
              "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
              "    fully commented lines are ignored by the parameter ``header`` but not by\n",
              "    ``skiprows``. For example, if ``comment=&#x27;#&#x27;``, parsing\n",
              "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``&#x27;a,b,c&#x27;`` being\n",
              "    treated as the header.\n",
              "encoding : str, optional, default &#x27;utf-8&#x27;\n",
              "    Encoding to use for UTF when reading/writing (ex. ``&#x27;utf-8&#x27;``). `List of Python\n",
              "    standard encodings\n",
              "    &lt;https://docs.python.org/3/library/codecs.html#standard-encodings&gt;`_ .\n",
              "\n",
              "encoding_errors : str, optional, default &#x27;strict&#x27;\n",
              "    How encoding errors are treated. `List of possible values\n",
              "    &lt;https://docs.python.org/3/library/codecs.html#error-handlers&gt;`_ .\n",
              "\n",
              "    .. versionadded:: 1.3.0\n",
              "\n",
              "dialect : str or csv.Dialect, optional\n",
              "    If provided, this parameter will override values (default or not) for the\n",
              "    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
              "    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
              "    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
              "    documentation for more details.\n",
              "on_bad_lines : {&#x27;error&#x27;, &#x27;warn&#x27;, &#x27;skip&#x27;} or Callable, default &#x27;error&#x27;\n",
              "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
              "    Allowed values are :\n",
              "\n",
              "    - ``&#x27;error&#x27;``, raise an Exception when a bad line is encountered.\n",
              "    - ``&#x27;warn&#x27;``, raise a warning when a bad line is encountered and skip that line.\n",
              "    - ``&#x27;skip&#x27;``, skip bad lines without raising or warning when they are encountered.\n",
              "\n",
              "    .. versionadded:: 1.3.0\n",
              "\n",
              "    .. versionadded:: 1.4.0\n",
              "\n",
              "        - Callable, function with signature\n",
              "          ``(bad_line: list[str]) -&gt; list[str] | None`` that will process a single\n",
              "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
              "          If the function returns ``None``, the bad line will be ignored.\n",
              "          If the function returns a new ``list`` of strings with more elements than\n",
              "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
              "          Only supported when ``engine=&#x27;python&#x27;``\n",
              "\n",
              "    .. versionchanged:: 2.2.0\n",
              "\n",
              "        - Callable, function with signature\n",
              "          as described in `pyarrow documentation\n",
              "          &lt;https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
              "          #pyarrow.csv.ParseOptions.invalid_row_handler&gt;`_ when ``engine=&#x27;pyarrow&#x27;``\n",
              "\n",
              "delim_whitespace : bool, default False\n",
              "    Specifies whether or not whitespace (e.g. ``&#x27; &#x27;`` or ``&#x27;\\t&#x27;``) will be\n",
              "    used as the ``sep`` delimiter. Equivalent to setting ``sep=&#x27;\\s+&#x27;``. If this option\n",
              "    is set to ``True``, nothing should be passed in for the ``delimiter``\n",
              "    parameter.\n",
              "\n",
              "    .. deprecated:: 2.2.0\n",
              "        Use ``sep=&quot;\\s+&quot;`` instead.\n",
              "low_memory : bool, default True\n",
              "    Internally process the file in chunks, resulting in lower memory use\n",
              "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
              "    types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
              "    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
              "    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
              "    chunks. (Only valid with C parser).\n",
              "memory_map : bool, default False\n",
              "    If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
              "    directly onto memory and access the data directly from there. Using this\n",
              "    option can improve performance because there is no longer any I/O overhead.\n",
              "float_precision : {&#x27;high&#x27;, &#x27;legacy&#x27;, &#x27;round_trip&#x27;}, optional\n",
              "    Specifies which converter the C engine should use for floating-point\n",
              "    values. The options are ``None`` or ``&#x27;high&#x27;`` for the ordinary converter,\n",
              "    ``&#x27;legacy&#x27;`` for the original lower precision pandas converter, and\n",
              "    ``&#x27;round_trip&#x27;`` for the round-trip converter.\n",
              "\n",
              "storage_options : dict, optional\n",
              "    Extra options that make sense for a particular storage connection, e.g.\n",
              "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
              "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
              "    URLs (e.g. starting with &quot;s3://&quot;, and &quot;gcs://&quot;) the key-value pairs are\n",
              "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
              "    details, and for more examples on storage options refer `here\n",
              "    &lt;https://pandas.pydata.org/docs/user_guide/io.html?\n",
              "    highlight=storage_options#reading-writing-remote-files&gt;`_.\n",
              "\n",
              "dtype_backend : {&#x27;numpy_nullable&#x27;, &#x27;pyarrow&#x27;}, default &#x27;numpy_nullable&#x27;\n",
              "    Back-end data type applied to the resultant :class:`DataFrame`\n",
              "    (still experimental). Behaviour is as follows:\n",
              "\n",
              "    * ``&quot;numpy_nullable&quot;``: returns nullable-dtype-backed :class:`DataFrame`\n",
              "      (default).\n",
              "    * ``&quot;pyarrow&quot;``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
              "      DataFrame.\n",
              "\n",
              "    .. versionadded:: 2.0\n",
              "\n",
              "Returns\n",
              "-------\n",
              "DataFrame or TextFileReader\n",
              "    A comma-separated values (csv) file is returned as two-dimensional\n",
              "    data structure with labeled axes.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; pd.read_csv(&#x27;data.csv&#x27;)  # doctest: +SKIP</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 868);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MNs11HhOxhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}